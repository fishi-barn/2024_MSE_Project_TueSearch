{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 4271 - Group Project\n",
    "\n",
    "Issued: June 11, 2024\n",
    "\n",
    "Due: July 22, 2024\n",
    "\n",
    "Please submit a link to your code base (ideally with a branch that does not change anymore after the submission deadline) and your 4-page report via email to carsten.eickhoff@uni-tuebingen.de by the due date. One submission per team.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors**: Stephan Amann, Tanja Huber, Markus Potthast, Tina Truong\n",
    "\n",
    "**Date**: 22.07.2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T15:24:22.281522Z",
     "start_time": "2024-07-21T15:24:06.533994Z"
    }
   },
   "outputs": [],
   "source": [
    "# python 3.10 necessary\n",
    "!python --version\n",
    "\n",
    "# requirements:\n",
    "# - numpy, sentence-transformers, torch\n",
    "# - spacy, asyncio, httpx, loguru, selectolax\n",
    "# - pyopenssl, streamlitm, matplotlib\n",
    "%pip install -U -q setuptools wheel\n",
    "%pip install -q numpy sentence-transformers torch\n",
    "%pip install -q fasttext-langdetect\n",
    "%pip install -U -q spacy\n",
    "%pip install -q asyncio httpx loguru selectolax pyopenssl \n",
    "%pip install -q streamlit\n",
    "%pip install -q matplotlib \n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Need to download index which is a large file => need to install git lfs\n",
    "# using PackageCloud.io\n",
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "# for debian based linux systems\n",
    "!sudo apt-get install git-lfs\n",
    "# or download tar.gz and install manually\n",
    "# https://github.com/git-lfs/git-lfs/releases/download/v3.5.1/git-lfs-linux-amd64-v3.5.1.tar.gz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Web Crawling & Indexing\n",
    "Crawl the web to discover **English content related to Tübingen**. The crawled content should be stored locally. If interrupted, your crawler should be able to re-start and pick up the crawling process at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 13:21:36.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcrawler\u001b[0m:\u001b[36mtask_getter\u001b[0m:\u001b[36m176\u001b[0m - \u001b[1mLimit of 10 tasks reached for frontier BOTH\u001b[0m\n",
      "\u001b[32m2024-07-23 13:21:36.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcrawler\u001b[0m:\u001b[36mtask_getter\u001b[0m:\u001b[36m176\u001b[0m - \u001b[1mLimit of 10 tasks reached for frontier BOTH\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 13:21:36.600\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mcrawler\u001b[0m:\u001b[36mfetch\u001b[0m:\u001b[36m120\u001b[0m - \u001b[33m\u001b[1mFetching https://www.kyb.tuebingen.mpg.de/de/forschung/abt/bs.html failed with Client error '404 Not Found' for url 'https://www.kyb.tuebingen.mpg.de/de/forschung/abt/bs.html'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\u001b[0m\n",
      "\u001b[32m2024-07-23 13:21:36.679\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mcrawler\u001b[0m:\u001b[36mfetch\u001b[0m:\u001b[36m125\u001b[0m - \u001b[33m\u001b[1mRetrying https://www.kyb.tuebingen.mpg.de/de/forschung/abt/bs.html after 0 attempts.\u001b[0m\n",
      "\u001b[32m2024-07-23 13:21:36.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcrawler\u001b[0m:\u001b[36mfetch\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1mFetched https://www.kyb.tuebingen.mpg.de/adress successfully\u001b[0m\n",
      "\u001b[32m2024-07-23 13:21:36.712\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcrawler\u001b[0m:\u001b[36mcrawl\u001b[0m:\u001b[36m144\u001b[0m - \u001b[34m\u001b[1mProcessing https://www.kyb.tuebingen.mpg.de/adress ...\u001b[0m\n",
      "\u001b[32m2024-07-23 13:21:36.727\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mscraper\u001b[0m:\u001b[36mlanguage_correct\u001b[0m:\u001b[36m92\u001b[0m - \u001b[33m\u001b[1mLanguage Detection: {'lang': 'en', 'score': 0.5984677076339722}\u001b[0m\n",
      "\u001b[32m2024-07-23 13:21:36.992\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mscraper\u001b[0m:\u001b[36mget_urls\u001b[0m:\u001b[36m268\u001b[0m - \u001b[32m\u001b[1mFound 38 new URLs.\u001b[0m\n",
      "\u001b[32m2024-07-23 13:21:36.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata\u001b[0m:\u001b[36mremove_url\u001b[0m:\u001b[36m190\u001b[0m - \u001b[1mFrontier: Removing url https://www.kyb.tuebingen.mpg.de/adress\u001b[0m\n",
      "\u001b[32m2024-07-23 13:21:36.995\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcrawler\u001b[0m:\u001b[36mfetch\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mFetch operation for https://www.kyb.tuebingen.mpg.de/info-master was cancelled.\u001b[0m\n",
      "\u001b[32m2024-07-23 13:21:36.995\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcrawler\u001b[0m:\u001b[36mfetch\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mFetch operation for https://www.kyb.tuebingen.mpg.de/talks-events was cancelled.\u001b[0m\n",
      "\u001b[32m2024-07-23 13:21:36.996\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcrawler\u001b[0m:\u001b[36mfetch\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mFetch operation for https://www.kyb.tuebingen.mpg.de/de/forschung/abt/bs.html was cancelled.\u001b[0m\n",
      "\u001b[32m2024-07-23 13:21:36.996\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcrawler\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mKeyboardInterrupt received, stopping crawler and performing cleanup...\u001b[0m\n",
      "\u001b[32m2024-07-23 13:21:37.653\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mcrawler\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m232\u001b[0m - \u001b[32m\u001b[1mCleanup completed, exiting.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Crawl and Scrape\n",
    "from crawler import WebCrawler\n",
    "crawler = WebCrawler()\n",
    "await crawler.run()\n",
    "# Use keyboaad interrupt (such as CTRL-C) to stop the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T15:30:58.217945Z",
     "start_time": "2024-07-21T15:24:23.756545Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steph/miniconda3/envs/MSE/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus is loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents: 100%|██████████| 5057/5057 [00:41<00:00, 121.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents indexed.\n",
      "Computing topic distributions...\n",
      "Topic distributions computed.\n",
      "Saved index, document lengths, document embeddings, and topic distributions to files.\n"
     ]
    }
   ],
   "source": [
    "# Indexing of the processed corpus\n",
    "from index import Index\n",
    "index = Index()\n",
    "index.index_documents()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Query Processing \n",
    "Process a textual query and return the 100 most relevant documents from your index. Please incorporate **at least one retrieval model innovation** that goes beyond BM25 or TF-IDF. Please allow for queries to be entered either individually in an interactive user interface (see also #3 below), or via a batch file containing multiple queries at once. The batch file will be formatted to have one query per line, listing the query number, and query text as tab-separated entries. An example of the batch file for the first two queries looks like this:\n",
    "\n",
    "```\n",
    "1   tübingen attractions\n",
    "2   food and drinks\n",
    "```\n",
    "\n",
    "Single queries and query files are possible, use\n",
    "- `tse.search(\"query\")`\n",
    "- `tse.search_from_file(\"queries.txt\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T15:31:16.119116Z",
     "start_time": "2024-07-21T15:30:58.219940Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuna/mambaforge/envs/tueSearch/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus is loaded!\n",
      "Rank 1: ('5025', 0.8245178797922322)\n",
      "Rank 2: ('4985', 0.8017312021731078)\n",
      "Rank 3: ('1035', 0.7486135043482343)\n",
      "Rank 4: ('4958', 0.6343139799244908)\n",
      "Rank 5: ('85', 0.6217061469320131)\n",
      "Rank 6: ('1844', 0.619331792626966)\n",
      "Rank 7: ('4899', 0.5971960687418162)\n",
      "Rank 8: ('340', 0.5905999009024203)\n",
      "Rank 9: ('2408', 0.5822676438407626)\n",
      "Rank 10: ('1261', 0.5683328730293123)\n"
     ]
    }
   ],
   "source": [
    "from tue_search import TuebingenSearchEngine as TSE \n",
    "tse = TSE()\n",
    "\n",
    "# Example query\n",
    "# 1. single query\n",
    "query_result = tse.search(\"Wilhelm Schickard\") # Friedrich Miescher\n",
    "# 2. Query batch from file\n",
    "# query_result = tse.search_from_file(\"queries.txt\")\n",
    "\n",
    "# Prints the top k results. Structure is in form of (docID, score).\n",
    "k = 10\n",
    "for i in range(k):\n",
    "    print(f\"Rank {i} : {query_result[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Search Result Presentation\n",
    "Once you have a result set, we want to return it to the searcher in two ways: a) in an interactive user interface. For this user interface, please think of **at least one innovation** that goes beyond the traditional 10-blue-links interface that most commercial search engines employ. b) as a text file used for batch performance evaluation. The text file should be formatted to produce one ranked result per line, listing the query number, rank position, document URL and relevance score as tab-separated entries. An example of the first three lines of such a text file looks like this:\n",
    "\n",
    "```\n",
    "1   1   https://www.tuebingen.de/en/3521.html   0.725\n",
    "1   2   https://www.komoot.com/guide/355570/castles-in-tuebingen-district   0.671\n",
    "1   3   https://www.unimuseum.uni-tuebingen.de/en/museum-at-hohentuebingen-castle   0.529\n",
    "...\n",
    "1   100 https://www.tuebingen.de/en/3536.html   0.178\n",
    "2   1   https://www.tuebingen.de/en/3773.html   0.956\n",
    "2   2   https://www.tuebingen.de/en/4456.html   0.797\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T15:31:50.732724Z",
     "start_time": "2024-07-21T15:31:16.120116Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 6/6 [00:32<00:00,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries processed and results saved to query_results.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tse.search_from_file(\"queries.txt\") # given a file with queries, will result in a result batch called query_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T11:55:21.104436Z",
     "start_time": "2024-07-22T11:55:21.048444Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:8501\n",
      "  Network URL: http://192.168.1.216:8501\n",
      "\n",
      "corpus is loaded!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# to start \n",
    "process = subprocess.Popen(['streamlit', 'run', 'app.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T12:40:49.115668Z",
     "start_time": "2024-07-22T12:40:49.086668Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Stopping...\n"
     ]
    }
   ],
   "source": [
    "# to stop\n",
    "process.terminate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Performance Evaluation \n",
    "We will evaluate the performance of our search systems on the basis of five queries. Two of them are avilable to you now for engineering purposes:\n",
    "- `tübingen attractions`\n",
    "- `food and drinks`\n",
    "\n",
    "The remaining three queries will be given to you during our final session on July 23rd. Please be prepared to run your systems and produce a single result file for all five queries live in class. That means you should aim for processing times of no more than ~1 minute per query. We will ask you to send carsten.eickhoff@uni-tuebingen.de that file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "Your final projects will be graded along the following criteria:\n",
    "- 25% Code correctness and quality (to be delivered on this sheet)\n",
    "- 25% Report (4 pages, PDF, explanation and justification of your design choices)\n",
    "- 25% System performance (based on how well your system performs on the 5 queries relative to the other teams in terms of nDCG)\n",
    "- 15% Creativity and innovativeness of your approach (in particular with respect to your search system #2 and user interface #3 innovations)\n",
    "- 10% Presentation quality and clarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permissible libraries\n",
    "You can use any general-puprose ML and NLP libraries such as scipy, numpy, scikit-learn, spacy, nltk, but please stay away from dedicated web crawling or search engine toolkits such as scrapy, whoosh, lucene, terrier, galago and the likes. Pretrained models are fine to use as part of your system, as long as they have not been built/trained for retrieval. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
